{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main points\n",
    "* Solution should be reasonably simple because the contest is only 24 hours long \n",
    "* Metric is based on the prediction of clicked pictures one week ahead, so clicks are the most important information\n",
    "* More recent information is more important\n",
    "* Only pictures that were shown to a user could be clicked, so pictures popularity is important\n",
    "* Metric is MAPK@100\n",
    "* Link https://contest.yandex.ru/contest/12899/problems (Russian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "* Build a classic recommending system based on user click history\n",
    "* Only use recent days of historical data\n",
    "* Take into consideration projected picture popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic constants\n",
    "### ALS recommending system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factors for ALS\n",
    "factors_count=100\n",
    "\n",
    "# Last days of click history used\n",
    "trail_days=14 \n",
    "\n",
    "# number of best candidates generated by ALS \n",
    "output_candidates_count=2000 \n",
    "\n",
    "# Last days of history with more weight\n",
    "last_days=1\n",
    "\n",
    "# Coefficient for additional weight\n",
    "last_days_weight=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular pictures prediction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "lightgbm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_model = lightgbm.LGBMRegressor(seed=0)\n",
    "heuristic_alpha = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.8'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import implicit\n",
    "implicit.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = pd.read_csv('Blitz/test_users.csv')\n",
    "data = pd.read_csv('Blitz/train_clicks.csv', parse_dates=['day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split last 7 days to calculate clicks similar to test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, target_week = (\n",
    "    data[data.day <= datetime.datetime(2019, 3, 17)].copy(),\n",
    "    data[data.day > datetime.datetime(2019, 3, 17)],\n",
    ")\n",
    "train.day.nunique(), target_week.day.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = train.day.max()\n",
    "train.loc[:, 'delta_days'] = 1 + (last_date - train.day).apply(lambda d: d.days)\n",
    "\n",
    "last_date = data.day.max()\n",
    "data.loc[:, 'delta_days'] = 1 + (last_date - data.day).apply(lambda d: d.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picture_features(data):\n",
    "    \"\"\"Generating clicks count for every picture in last days\"\"\"\n",
    "    days = range(1, 3)\n",
    "    features = []\n",
    "    names = []\n",
    "    for delta_days in days:\n",
    "        features.append(\n",
    "            data[(data.delta_days == delta_days)].groupby(['picture_id'])['user_id'].count()\n",
    "        )\n",
    "        names.append('%s_%d' % ('click', delta_days))\n",
    "        \n",
    "    features = pd.concat(features, axis=1).fillna(0)\n",
    "    features.columns = names\n",
    "    features = features.reindex(data.picture_id.unique())\n",
    "    return features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "click_1    0.046854\n",
       "click_2    0.046599\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = picture_features(train)\n",
    "X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1070979,), 0.15237180187473331)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clicks_count(data, index):\n",
    "    return data.groupby('picture_id')['user_id'].count().reindex(index).fillna(0)\n",
    "    \n",
    "y = clicks_count(target_week, X.index)\n",
    "y.shape, y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model predicting popular pictures next week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
       "       n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "       reg_alpha=0.0, reg_lambda=0.0, seed=0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popularity_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "click_1    0.042347\n",
       "click_2    0.038887\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = picture_features(data)\n",
    "X_test.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14701204450320687"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['p'] = popularity_model.predict(X_test)\n",
    "X_test.loc[X_test['p'] < 0, 'p'] = 0\n",
    "X_test['p'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dict with predicted clicks for every picture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prediction would be used to correct recommender score\n",
    "picture = dict(X_test['p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate prediction using ALS approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = \"1\"\n",
    "\n",
    "def als_baseline(\n",
    "    train, test_users, \n",
    "    factors_n, last_days, trail_days, output_candidates_count, last_days_weight\n",
    "):\n",
    "    train = train[train.delta_days <= trail_days].drop_duplicates([\n",
    "        'user_id', 'picture_id'\n",
    "    ])\n",
    "    \n",
    "    users = train.user_id\n",
    "    items = train.picture_id\n",
    "    weights = 1 + last_days_weight * (train.delta_days <= last_days)\n",
    "    \n",
    "    user_item = coo_matrix((weights, (users, items)))\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=factors_n, iterations=factors_n)\n",
    "    model.fit(user_item.T.tocsr())\n",
    "    \n",
    "    user_item_csr = user_item.tocsr()\n",
    "    \n",
    "    rows = []\n",
    "    for user_id in tqdm.tqdm_notebook(test_users.user_id.values):\n",
    "        items = [(picture_id, score) for picture_id, score in model.recommend(user_id, user_item_csr, N=output_candidates_count)]\n",
    "        rows.append(items)\n",
    "\n",
    "    test_users['predictions_full'] = [\n",
    "        p\n",
    "        for p, user_id in zip(\n",
    "            rows,\n",
    "            test_users.user_id.values\n",
    "        )\n",
    "    ]\n",
    "    test_users['predictions'] = [\n",
    "        [x[0] for x in p]\n",
    "        for p, user_id in zip(\n",
    "            rows,\n",
    "            test_users.user_id.values\n",
    "        )\n",
    "    ]\n",
    "    return test_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100.0/100 [11:00<00:00,  6.78s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23990e61be5846a19c695fb94be2ca4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1380), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_users = als_baseline(\n",
    "    data, test_users, factors_count, last_days, trail_days, output_candidates_count, last_days_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate history clicks to exclude them from results. Such clicks are excluded from test set according to task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked = data.groupby('user_id').agg({'picture_id': set})\n",
    "\n",
    "def substract_clicked(p, c):\n",
    "    filtered = [picture for picture in p if picture not in c][:100]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristical approach to reweight ALS score according to picture predicted popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender returns (picture, score) pairs sorted decreasing for every user.\n",
    "\n",
    "For every user we replace picture $score_p$ with $score_p \\cdot (1 + popularity_{p})^{0.2}$\n",
    "\n",
    "$popularity_{p}$ - popularity predicted for this picture for next week\n",
    "\n",
    "This slightly moves popular pictures to the top of list for every user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "rows = test_users['predictions_full']\n",
    "\n",
    "def correct_with_popularity(items, picture, alpha):\n",
    "    return sorted([\n",
    "        (score * (1 + picture.get(picture_id, 0)) ** alpha, picture_id, score, picture.get(picture_id, 0)) \n",
    "        for picture_id, score in items], reverse=True\n",
    "    )\n",
    "\n",
    "corrected_rows = [\n",
    "    [x[1] for x in correct_with_popularity(items, picture, heuristic_alpha)]\n",
    "    for items in rows\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users['predictions'] = [\n",
    "    ' '.join(map(str,\n",
    "        substract_clicked(p, {} if user_id not in clicked.index else clicked.loc[user_id][0])\n",
    "    ))\n",
    "    for p, user_id in zip(\n",
    "        corrected_rows,\n",
    "        test_users.user_id.values\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users[['user_id', 'predictions']].to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
